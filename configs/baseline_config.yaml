model:
  name: "google/flan-t5-base"
  max_input_length: 512
  max_output_length: 128

lora:
  r: 12
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q", "v", "k", "o", "wi_0", "wi_1", "wo"]

training:
  num_train_epochs: 4
  per_device_train_batch_size: 6
  per_device_eval_batch_size: 6
  learning_rate: 8e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  eval_steps: 382
  save_steps: 764
  early_stopping_patience: 3

evaluation:
  metrics: ["rouge1", "rouge2", "rougeL", "entity_coverage"]
  demographic_groups: ["male_adult", "male_pediatric", "male_elderly", 
                      "female_adult", "female_pediatric", "female_elderly"]
