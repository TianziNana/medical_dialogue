
---

#  README 

### Abstract

This project develops a **fairness-aware medical summarization model** built on *Flan-T5-base*. Using augmented clinical notes and doctor-patient dialogues, we fine-tune a lightweight sequence-to-sequence model that generates concise, structured summaries of clinical encounters. Beyond evaluating standard text quality metrics (ROUGE-L, BERTScore), we introduce a **fairness audit framework** that measures subgroup disparities (e.g., gender, age) and counterfactual stability (e.g., whether advice changes if patient demographics are swapped). We further explore lightweight mitigation strategies—reweighting, counterfactual data augmentation, and safety-guided post-processing—that reduce subgroup performance gaps without sacrificing overall summarization quality. Our results highlight the importance of **fair and equitable clinical NLP**, showing that medical summarization models can be both accurate and socially responsible.

---

### Introduction & Background

Clinical summarization is a cornerstone task in medical natural language processing (NLP). Physicians and healthcare workers are often overwhelmed by lengthy, unstructured notes and conversations. Automated summarization systems promise to transform this workflow by **extracting essential clinical insights**—such as symptoms, diagnoses, and treatment recommendations—into concise formats. Such systems can improve patient communication, support decision-making, and reduce documentation burdens.

Despite recent progress in neural summarization, most existing systems are optimized solely for **accuracy**, overlooking whether model performance is **equitable across patient subgroups**. This omission is especially problematic in healthcare, where bias against certain demographics—such as women, adolescents, or elderly patients—can reinforce existing inequities. Prior work has shown that demographic imbalances in training data often translate into systematic performance gaps in downstream predictions.

Our project bridges this gap by explicitly combining **medical summarization** with **fairness-aware machine learning**. By extending standard training pipelines with fairness auditing and mitigation, we aim to demonstrate that it is possible to build models that are not only *high-performing* but also *socially responsible*.

---

### Problem Statement

This work investigates two guiding questions:

1. **Can a general-purpose pretrained model (Flan-T5-base) be adapted to perform accurate, domain-specific medical summarization on real-world clinical datasets?**
2. **How can fairness be systematically measured and improved in clinical summarization tasks, ensuring equitable performance across patient demographics such as gender and age?**

To answer these, we:

* Fine-tune *Flan-T5-base* on augmented clinical notes and medical dialogue datasets.
* Propose a **fairness audit panel**, evaluating subgroup metrics (ROUGE, BERTScore, entity coverage) and counterfactual stability tests.
* Apply lightweight **fairness mitigation strategies**, including subgroup reweighting and counterfactual data augmentation.

Through this pipeline, we present a practical framework for developing **fairness-aware medical summarization models**, highlighting both technical contributions and societal relevance.

---

### Limitations

While our framework demonstrates promising results, several limitations remain:

* **Data Coverage**: The training data may not fully represent the diversity of real-world clinical conversations (e.g., underrepresentation of minority groups or rare conditions).
* **Evaluation Scope**: Our fairness evaluation focuses on gender and age, but other sensitive attributes (race, socioeconomic status) are not included due to dataset constraints.
* **Clinical Validity**: Summaries generated by the model are not validated by medical experts; they should not be used for real clinical decision-making without human oversight.
* **Model Scale**: We employ lightweight models (Flan-T5-base) suitable for Kaggle-scale training; larger models may achieve higher performance but introduce higher computational and fairness risks.

---

### Future Work

Building on this foundation, future research directions include:

* **Expanding Fairness Dimensions**: Extending subgroup analysis to race, socioeconomic background, and comorbidities to capture broader inequities.
* **Human-in-the-Loop Evaluation**: Incorporating feedback from clinicians to better assess both the accuracy and practical utility of generated summaries.
* **Cross-Dataset Generalization**: Testing fairness-aware summarization models across multiple datasets and healthcare domains (e.g., radiology, mental health).
* **Interactive Clinical AI Systems**: Integrating fairness-aware summarization into real-time clinical decision support tools, with safety guardrails and interpretability mechanisms.
* **Benchmark Contribution**: Releasing standardized fairness audit scripts and metrics to encourage reproducibility and facilitate community-wide benchmarking in clinical NLP.

---

┌──────────────────┐
│   数据准备       │
│ - 加载临床对话/病历 │
│ - 提取性别、年龄等 │
└──────────┬─────┘
           │
           ▼
┌──────────────────┐
│   基线训练       │
│ - 使用 Flan-T5-base │
│ - 医学摘要生成任务 │
│ - 评估质量指标   │
└──────────┬─────┘
           │
           ▼
┌──────────────────┐
│   公平性审计     │
│ - 子群体评估(F/M,年龄段)│
│ - 反事实公平性测试 │
└──────────┬─────┘
           │
           ▼
┌──────────────────┐
│   公平性缓解     │
│ - 重加权采样     │
│ - 反事实数据增强 │
│ - 安全提示后处理 │
└──────────┬─────┘
           │
           ▼
┌──────────────────┐
│   结果对比       │
│ - 总体性能 vs 公平性 │
│ - 子群体差距缩小 │
│ - 泛化性验证     │
└──────────────────┘
